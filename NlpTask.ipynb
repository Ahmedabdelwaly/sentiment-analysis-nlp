{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyH/Umjd1SiAWq+7rzoSPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmedabdelwaly/sentiment-analysis-nlp/blob/main/NlpTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading"
      ],
      "metadata": {
        "id": "xGosvpfvuLPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "hJO7N1qVEfAp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSzOej6iuHc_",
        "outputId": "ce7ab403-9176-416f-c9e7-6d32584e8dd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Sentimental/Sentimental\"\n",
        "if os.path.exists(folder_path):\n",
        "    print(\"Nested folder exists. Listing files:\")\n",
        "    print(os.listdir(folder_path))\n",
        "else:\n",
        "    print(\"Nested folder not found! Check the folder structure in Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVMVJY11uJ9-",
        "outputId": "762eba41-a14a-49dd-d1e9-16f16b313bf1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested folder exists. Listing files:\n",
            "['training.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Sentimental/Sentimental/training.csv\"  # Adjust if needed\n",
        "df = pd.read_csv(file_path, encoding='latin-1', usecols=[0, 5], names=['Sentiment', 'Text'])"
      ],
      "metadata": {
        "id": "LRTmr_OZueUE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "w-JKqndsIeSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'] = df['Sentiment'].replace({0: 0, 4: 1})"
      ],
      "metadata": {
        "id": "POBuY3-p21Oq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "V5iGYs-_3D_6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr6Uiq2guWpd",
        "outputId": "60b3ef66-2b1f-4424-fb01-1a01b29c6515"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = {\n",
        "    \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\", \"that's\": \"that is\", \"what's\": \"what is\", \"where's\": \"where is\", \"who's\": \"who is\",\n",
        "    \"n't\": \" not\", \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\"\n",
        "}"
      ],
      "metadata": {
        "id": "LCDWemPCvBZj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "def expand_contractions(text):\n",
        "    for key, value in contractions.items():\n",
        "        text = text.replace(key, value)\n",
        "    return text\n",
        "\n",
        "# Improved Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)  # Expand contractions\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'#[A-Za-z0-9_]+', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)  # Remove non-alphabetic characters\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 2])  # Lemmatization & stopwords removal\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['Text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Balance the dataset using oversampling\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(df[['cleaned_text']], df['Sentiment'])\n",
        "df_balanced = pd.DataFrame({'cleaned_text': X_resampled['cleaned_text'], 'Sentiment': y_resampled})"
      ],
      "metadata": {
        "id": "vX44Pb-r2Boa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 10000\n",
        "max_length = 50\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df_balanced['cleaned_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df_balanced['cleaned_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df_balanced['Sentiment'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "TUSrkbqlzAEg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rnn Model"
      ],
      "metadata": {
        "id": "JGFM4wQUIYHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(max_words, 128, input_length=max_length),\n",
        "    SimpleRNN(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "2JdL-i9C391D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ba47f0-899b-47ac-b6d9-60775855eaa3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m727s\u001b[0m 36ms/step - accuracy: 0.6611 - loss: 0.6205 - val_accuracy: 0.7234 - val_loss: 0.5613\n",
            "Epoch 2/5\n",
            "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m695s\u001b[0m 34ms/step - accuracy: 0.7278 - loss: 0.5605 - val_accuracy: 0.6844 - val_loss: 0.5924\n",
            "Epoch 3/5\n",
            "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 34ms/step - accuracy: 0.7006 - loss: 0.5822 - val_accuracy: 0.5016 - val_loss: 0.6931\n",
            "Epoch 4/5\n",
            "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 34ms/step - accuracy: 0.6169 - loss: 0.6516 - val_accuracy: 0.6911 - val_loss: 0.5955\n",
            "Epoch 5/5\n",
            "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 34ms/step - accuracy: 0.6979 - loss: 0.5873 - val_accuracy: 0.7141 - val_loss: 0.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Cs4DCE4C4y",
        "outputId": "bc160139-e8bb-473c-f4bf-3de78453d73b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 7ms/step - accuracy: 0.7137 - loss: 0.5720\n",
            "Accuracy: 71.41%\n"
          ]
        }
      ]
    }
  ]
}